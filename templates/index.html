<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Voice Agent</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
      .audio-wave {
        position: absolute;
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%);
        width: 100%;
        height: 100%;
        border-radius: 50%;
        border: 2px solid rgba(255, 255, 255, 0.5);
        opacity: 0;
        z-index: -1;
      }
      .recording .audio-wave {
        animation: wave 1.5s infinite ease-out;
      }
      @keyframes wave {
        0% {
          width: 100%;
          height: 100%;
          opacity: 0.8;
        }
        100% {
          width: 200%;
          height: 200%;
          opacity: 0;
        }
      }
      #chat-container {
        scrollbar-width: thin;
      }
    </style>
  </head>
  <body
    class="bg-gray-900 text-white h-screen flex flex-col items-center justify-between p-4 overflow-hidden"
  >
    <!-- Chat Area -->
    <div
      id="chat-container"
      class="flex-1 w-full max-w-2xl overflow-y-auto space-y-4 p-4 rounded-lg bg-gray-800/50 backdrop-blur-sm mb-8 mt-4"
    >
      <div class="text-center text-gray-400 text-sm">
        Start talking to interact...
      </div>
    </div>

    <!-- Voice Activity Indicator -->
    <div id="voice-activity" class="w-full max-w-2xl mb-4">
      <!-- Speaking/Silence badge -->
      <div class="flex items-center justify-between mb-2">
        <div class="flex items-center gap-2">
          <div
            id="activityDot"
            class="w-3 h-3 rounded-full bg-gray-500 transition-colors duration-200"
          ></div>
          <span id="activityLabel" class="text-sm font-medium text-gray-400"
            >Idle</span
          >
        </div>
        <span id="dbDisplay" class="text-xs text-gray-500 font-mono"
          >-- dB</span
        >
      </div>
      <!-- dB level bar -->
      <div class="w-full h-2 bg-gray-700 rounded-full overflow-hidden">
        <div
          id="dbBar"
          class="h-full bg-green-500 rounded-full transition-all duration-150"
          style="width: 0%"
        ></div>
      </div>
      <!-- Threshold slider -->
      <div class="flex items-center gap-3 mt-2">
        <label
          for="thresholdSlider"
          class="text-xs text-gray-400 whitespace-nowrap"
          >Silence Threshold:</label
        >
        <input
          id="thresholdSlider"
          type="range"
          min="-100"
          max="0"
          value="-45"
          class="flex-1 h-1 accent-blue-500 cursor-pointer"
        />
        <span
          id="thresholdValue"
          class="text-xs text-gray-400 font-mono w-12 text-right"
          >-45 dB</span
        >
      </div>
    </div>

    <!-- Controls -->
    <div class="relative w-full flex flex-col items-center mb-12">
      <!-- Session Control Button -->
      <button
        id="sessionBtn"
        class="mb-8 px-6 py-2 rounded-full bg-green-600 hover:bg-green-500 font-bold transition-all shadow-md focus:outline-none"
      >
        Start Conversation
      </button>

      <div class="relative">
        <!-- Status Indicator (Non-clickable) -->
        <div
          id="statusCircle"
          class="w-20 h-20 rounded-full bg-gray-700 transition-all shadow-lg flex items-center justify-center z-10"
        >
          <svg
            id="micIcon"
            class="w-8 h-8 text-white"
            fill="none"
            stroke="currentColor"
            viewBox="0 0 24 24"
          >
            <path
              stroke-linecap="round"
              stroke-linejoin="round"
              stroke-width="2"
              d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z"
            ></path>
          </svg>
          <div id="waveEffect" class="audio-wave"></div>
        </div>
      </div>
      <div id="status" class="mt-4 text-gray-400 text-sm font-medium">Idle</div>
    </div>

    <script>
      const sessionBtn = document.getElementById("sessionBtn");
      const statusCircle = document.getElementById("statusCircle");
      const waveEffect = document.getElementById("waveEffect");
      const statusDiv = document.getElementById("status");
      const chatContainer = document.getElementById("chat-container");
      const micIcon = document.getElementById("micIcon");
      const activityDot = document.getElementById("activityDot");
      const activityLabel = document.getElementById("activityLabel");
      const dbDisplay = document.getElementById("dbDisplay");
      const dbBar = document.getElementById("dbBar");
      const thresholdSlider = document.getElementById("thresholdSlider");
      const thresholdValue = document.getElementById("thresholdValue");

      let ws;
      let mediaRecorder;
      let isRecording = false;
      let sessionActive = false; // Is the whole chat session on?
      let audioQueue = [];
      let isPlaying = false;
      let currentAudio = null;
      let silenceThreshold = -45; // default minDecibels threshold

      // Threshold slider â€” controls analyser.minDecibels (-100 to 0)
      thresholdSlider.addEventListener("input", (e) => {
        silenceThreshold = parseInt(e.target.value);
        thresholdValue.textContent = `${silenceThreshold} dB`;
        if (analyser) {
          analyser.minDecibels = silenceThreshold;
        }
      });

      function connectWebSocket() {
        const protocol = window.location.protocol === "https:" ? "wss:" : "ws:";
        ws = new WebSocket(`${protocol}//${window.location.host}/ws`);

        ws.onopen = () => {
          console.log("Connected to WebSocket");
          // Initialize mic/UI detection as soon as we connect (or on page load interaction)
          initMicUI();
        };

        ws.onmessage = async (event) => {
          const data = JSON.parse(event.data);

          if (data.type === "transcription") {
            addMessage("user", data.text);
          } else if (data.type === "response_text") {
            addMessage("ai", data.text);
            // If Browser TTS is NOT used, we wait for audio to restart listening
            // BUT if no audio comes (ElevenLabs off), we should restart here if sessionActive
          } else if (data.type === "audio") {
            audioQueue.push(data.data);
            playNextAudio();
          } else if (data.type === "browser_tts") {
            speakText(data.text);
          } else if (data.type === "status") {
            statusDiv.textContent = data.text;
            // When Backend sends "Ready", we can restart listening if session is still on
            if (data.text === "Ready" && sessionActive && !isRecording) {
              console.log("Backend Ready, restarting listening...");
              startRecording();
            }
          }
        };

        function speakText(text) {
          if ("speechSynthesis" in window) {
            const utterance = new SpeechSynthesisUtterance(text);
            utterance.onend = () => {
              if (sessionActive && !isRecording) {
                console.log("Browser TTS ended, restarting listening...");
                startRecording();
              }
            };
            window.speechSynthesis.speak(utterance);
          } else {
            console.error("Browser does not support text-to-speech.");
          }
        }
        ws.onclose = () => {
          console.log("Disconnected. Reconnecting...");
          setTimeout(connectWebSocket, 3000);
        };
      }

      connectWebSocket();

      function addMessage(role, text) {
        const div = document.createElement("div");
        div.className = `p-3 rounded-lg max-w-[80%] ${role === "user" ? "bg-blue-600 self-end ml-auto" : "bg-gray-700 self-start mr-auto"}`;
        div.textContent = text;
        chatContainer.appendChild(div);
        chatContainer.scrollTop = chatContainer.scrollHeight;
      }

      async function playNextAudio() {
        if (isPlaying || audioQueue.length === 0) {
          // If finished speaking and queue empty, restart listening if sessionActive
          if (
            !isPlaying &&
            audioQueue.length === 0 &&
            sessionActive &&
            !isRecording
          ) {
            // startRecording();
          }
          return;
        }
        isPlaying = true;
        const audioData = audioQueue.shift();
        const audio = new Audio("data:audio/mp3;base64," + audioData);
        audio.onended = () => {
          isPlaying = false;
          if (audioQueue.length === 0 && sessionActive && !isRecording) {
            console.log("ElevenLabs Audio ended, restarting listening...");
            startRecording();
          }
          playNextAudio();
        };
        currentAudio = audio;
        await audio.play();
      }

      sessionBtn.addEventListener("click", () => {
        if (!sessionActive) {
          sessionActive = true;
          sessionBtn.textContent = "Stop Session";
          sessionBtn.classList.replace("bg-green-600", "bg-red-600");
          // Check if we need to start the recorder
          if (mediaRecorder && mediaRecorder.state === "inactive") {
            mediaRecorder.start(2000);
            statusDiv.textContent = "Listening...";
            statusCircle.classList.replace("bg-gray-700", "bg-blue-600");
          }
        } else {
          sessionActive = false;
          sessionBtn.textContent = "Start Conversation";
          sessionBtn.classList.replace("bg-red-600", "bg-green-600");
          if (mediaRecorder && mediaRecorder.state === "recording") {
            mediaRecorder.stop();
          }
          statusDiv.textContent = "Session Stopped";
          statusCircle.classList.replace("bg-blue-600", "bg-gray-700");
        }
      });

      let audioContext;
      let analyser;
      let detectSoundRAF;
      let globalStream;

      async function initMicUI() {
        try {
          globalStream = await navigator.mediaDevices.getUserMedia({
            audio: true,
          });

          // Setup MediaRecorder for backend chunks
          mediaRecorder = new MediaRecorder(globalStream);
          mediaRecorder.ondataavailable = (event) => {
            if (
              sessionActive &&
              event.data.size > 0 &&
              ws.readyState === WebSocket.OPEN
            ) {
              const reader = new FileReader();
              reader.onload = () => {
                const base64Audio = reader.result.split(",")[1];
                ws.send(
                  JSON.stringify({ type: "audio_chunk", data: base64Audio }),
                );
              };
              reader.readAsDataURL(event.data);
            }
          };

          mediaRecorder.onstop = () => {
            if (sessionActive || statusDiv.textContent === "Processing...") {
              setTimeout(() => {
                if (ws.readyState === WebSocket.OPEN) {
                  ws.send(JSON.stringify({ type: "stop_recording" }));
                }
              }, 500);
            }
          };

          // Setup Web Audio API â€” frequency-based silence detection
          audioContext = new (
            window.AudioContext || window.webkitAudioContext
          )();
          const source = audioContext.createMediaStreamSource(globalStream);
          analyser = audioContext.createAnalyser();
          analyser.minDecibels = silenceThreshold;
          source.connect(analyser);

          const bufferLength = analyser.frequencyBinCount;
          const domainData = new Uint8Array(bufferLength);

          let wasSpeaking = false;
          let silenceStart = null;
          const SILENCE_DURATION_MS = 1500;

          const detectSound = () => {
            analyser.getByteFrequencyData(domainData);

            let soundDetected = false;
            let maxVal = 0;
            for (let i = 0; i < bufferLength; i++) {
              if (domainData[i] > maxVal) maxVal = domainData[i];
              if (domainData[i] > 0) soundDetected = true;
            }

            const level = (maxVal / 255) * 100;

            // UI always works independently of session
            activityDot.className = `w-3 h-3 rounded-full transition-colors duration-200 ${soundDetected ? "bg-green-400" : "bg-red-400"}`;
            activityLabel.textContent = soundDetected
              ? "ðŸ—£ï¸ Speaking"
              : "ðŸ¤« Silence";
            activityLabel.className = `text-sm font-medium ${soundDetected ? "text-green-400" : "text-red-400"}`;
            dbDisplay.textContent = `max: ${maxVal}`;
            dbBar.style.width = `${level}%`;
            dbBar.className = `h-full rounded-full transition-all duration-150 ${soundDetected ? "bg-green-500" : "bg-red-500"}`;

            // Session-specific logic (Auto-stop & visuals)
            if (sessionActive) {
              if (soundDetected) {
                wasSpeaking = true;
                silenceStart = null;
                if (statusDiv.textContent !== "Speaking...") {
                  statusDiv.textContent = "Listening...";
                  statusCircle.classList.add("recording");
                  statusCircle.classList.replace("bg-gray-700", "bg-blue-600");
                }
              } else if (wasSpeaking && !soundDetected) {
                if (!silenceStart) {
                  silenceStart = Date.now();
                } else if (Date.now() - silenceStart >= SILENCE_DURATION_MS) {
                  console.log("Auto-stop detected after speech");
                  wasSpeaking = false;
                  silenceStart = null;
                  if (mediaRecorder.state === "recording") {
                    mediaRecorder.stop();
                  }
                  statusDiv.textContent = "Processing...";
                  statusCircle.classList.remove("recording");
                  statusCircle.classList.replace("bg-blue-600", "bg-gray-700");
                }
              }
            } else {
              wasSpeaking = false;
              silenceStart = null;
            }

            detectSoundRAF = window.requestAnimationFrame(detectSound);
          };

          detectSoundRAF = window.requestAnimationFrame(detectSound);
        } catch (err) {
          console.error("Microphone error:", err);
          statusDiv.textContent = "Microphone Error";
        }
      }

      function startRecording() {
        if (
          sessionActive &&
          mediaRecorder &&
          mediaRecorder.state === "inactive"
        ) {
          mediaRecorder.start(2000);
          statusDiv.textContent = "Listening...";
          statusCircle.classList.add("recording");
          statusCircle.classList.replace("bg-gray-700", "bg-blue-600");
        }
      }

      function stopRecording() {
        if (mediaRecorder && mediaRecorder.state === "recording") {
          mediaRecorder.stop();
        }
      }
    </script>
  </body>
</html>
